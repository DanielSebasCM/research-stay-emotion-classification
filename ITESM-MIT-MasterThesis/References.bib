@article{MANTYLA201816,
  title    = {The evolution of sentiment analysis—A review of research topics, venues, and top cited papers},
  journal  = {Computer Science Review},
  volume   = {27},
  pages    = {16-32},
  year     = {2018},
  issn     = {1574-0137},
  doi      = {https://doi.org/10.1016/j.cosrev.2017.10.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S1574013717300606},
  author   = {Mika V. Mäntylä and Daniel Graziotin and Miikka Kuutila},
  keywords = {Sentiment analysis, Opinion mining, Bibliometric study, Text mining, Literature review, Topic modeling, Latent Dirichlet Allocation, Qualitative analysis},
  abstract = {Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990’s. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis.}
}

@article{REUSENS2024124302,
  title    = {Evaluating text classification: A benchmark study},
  journal  = {Expert Systems with Applications},
  volume   = {254},
  pages    = {124302},
  year     = {2024},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2024.124302},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417424011680},
  author   = {Manon Reusens and Alexander Stevens and Jonathan Tonglet and Johannes {De Smedt} and Wouter Verbeke and Seppe {vanden Broucke} and Bart Baesens},
  keywords = {Benchmark, Text classification, RoBERTa, Bidirectional LSTM, Natural language processing, Machine learning},
  abstract = {This paper presents an impartial and extensive benchmark for text classification involving five different text classification tasks, 20 datasets, 11 different model architectures, and 42,800 algorithm runs. The five text classification tasks are fake news classification, topic detection, emotion detection, polarity detection, and sarcasm detection. While in practice, especially in Natural Language Processing (NLP), research tends to focus on the most sophisticated models, we hypothesize that this is not always necessary. Therefore, our main objective is to investigate whether the largest state-of-the-art (SOTA) models are always preferred, or in what cases simple methods can compete with complex models, i.e. for which dataset specifications and classification tasks. We assess the performance of different methods with varying complexity, ranging from simple statistical and machine learning methods to pretrained transformers like robustly optimized BERT (Bidirectional Encoder Representations from Transformers) pretraining approach (RoBERTa). This comprehensive benchmark is lacking in existing literature, with research mainly comparing similar types of methods. Furthermore, with increasing awareness of the ecological impacts of extensive computational resource usage, this comparison is both critical and timely.We find that overall, bidirectional long short-term memory (LSTM) networks are ranked as the best-performing method albeit not statistically significantly better than logistic regression and RoBERTa. Overall, we cannot conclude that simple methods perform worse although this depends mainly on the classification task. Concretely, we find that for fake news classification and topic detection, simple techniques are the best-ranked models and consequently, it is not necessary to train complicated neural network architectures for these classification tasks. Moreover, we also find a negative correlation between F1 performance and complexity for the smallest datasets (with dataset size less than 10,000). Finally, the different models’ results are analyzed in depth to explain the model decisions, which is an increasing requirement in the field of text classification.}
}
@book{ortony2022cognitive,
  title     = {The cognitive structure of emotions},
  author    = {Ortony, Andrew and Clore, Gerald L and Collins, Allan},
  year      = {2022},
  publisher = {Cambridge university press}
}


@article{hornik1989multilayer,
  title     = {Multilayer feedforward networks are universal approximators},
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  year      = {1989},
  publisher = {Elsevier}
}

@article{lieskovska2021review,
  title     = {A review on speech emotion recognition using deep learning and attention mechanism},
  author    = {Lieskovsk{\'a}, Eva and Jakubec, Maro{\v{s}} and Jarina, Roman and Chmul{\'\i}k, Michal},
  journal   = {Electronics},
  volume    = {10},
  number    = {10},
  pages     = {1163},
  year      = {2021},
  publisher = {MDPI}
}

@online{RNN,
  author = {{fdeloche}},
  title  = {Recurrent neural network unfold},
  year   = {2017},
  url    = {https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg},
  note   = {[Online; accessed 29-november-2024]}
}

@online{LSTM,
  author = {{fdeloche}},
  title  = {Long Short-Term Memory},
  year   = {2017},
  url    = {https://commons.wikimedia.org/wiki/File:Long_Short-Term_Memory.svg},
  note   = {[Online; accessed 29-november-2024]}
}

@article{hochreiter1997long,
  title   = {Long Short-term Memory},
  author  = {Hochreiter, S},
  journal = {Neural Computation MIT-Press},
  year    = {1997}
}

@article{10409495,
  author   = {Maruf, Abdullah Al and Khanam, Fahima and Haque, Md. Mahmudul and Jiyad, Zakaria Masud and Mridha, M. F. and Aung, Zeyar},
  journal  = {IEEE Access},
  title    = {Challenges and Opportunities of Text-Based Emotion Detection: A Survey},
  year     = {2024},
  volume   = {12},
  number   = {},
  pages    = {18416-18450},
  keywords = {Emotion recognition;Surveys;Social networking (online);Machine learning;Depression;Feature extraction;Business;Text analysis;Data models;Performance evaluation;Psychology;Text-based emotion detection;datasets;machine learning models;performance metrics;challenges and emerging trends},
  doi      = {10.1109/ACCESS.2024.3356357}
}


@inproceedings{saravia-etal-2018-carer,
    title = "{CARER}: Contextualized Affect Representations for Emotion Recognition",
    author = "Saravia, Elvis  and
      Liu, Hsien-Chi Toby  and
      Huang, Yen-Hao  and
      Wu, Junlin  and
      Chen, Yi-Shin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1404",
    doi = "10.18653/v1/D18-1404",
    pages = "3687--3697",
    abstract = "Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.",
}

@misc{nidula_elgiriyewithana_2024,
	title={Emotions},
	url={https://www.kaggle.com/dsv/7563141},
	DOI={10.34740/KAGGLE/DSV/7563141},
	publisher={Kaggle},
	author={Nidula Elgiriyewithana},
	year={2024}
}