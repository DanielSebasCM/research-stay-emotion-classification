{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"love\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"anger\"]\n",
    "\n",
    "stop_words = set(\n",
    "    [\n",
    "        \"i\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"myself\",\n",
    "        \"we\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"ourselves\",\n",
    "        \"you\",\n",
    "        \"you're\",\n",
    "        \"you've\",\n",
    "        \"you'll\",\n",
    "        \"you'd\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"yourself\",\n",
    "        \"yourselves\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"himself\",\n",
    "        \"she\",\n",
    "        \"she's\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"herself\",\n",
    "        \"it\",\n",
    "        \"it's\",\n",
    "        \"its\",\n",
    "        \"itself\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "        \"themselves\",\n",
    "        \"what\",\n",
    "        \"which\",\n",
    "        \"who\",\n",
    "        \"whom\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"that'll\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"am\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"being\",\n",
    "        \"have\",\n",
    "        \"has\",\n",
    "        \"had\",\n",
    "        \"having\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"did\",\n",
    "        \"doing\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"the\",\n",
    "        \"and\",\n",
    "        \"but\",\n",
    "        \"if\",\n",
    "        \"or\",\n",
    "        \"because\",\n",
    "        \"as\",\n",
    "        \"until\",\n",
    "        \"while\",\n",
    "        \"of\",\n",
    "        \"at\",\n",
    "        \"by\",\n",
    "        \"for\",\n",
    "        \"with\",\n",
    "        \"about\",\n",
    "        \"against\",\n",
    "        \"between\",\n",
    "        \"into\",\n",
    "        \"through\",\n",
    "        \"during\",\n",
    "        \"before\",\n",
    "        \"after\",\n",
    "        \"above\",\n",
    "        \"below\",\n",
    "        \"to\",\n",
    "        \"from\",\n",
    "        \"up\",\n",
    "        \"down\",\n",
    "        \"in\",\n",
    "        \"out\",\n",
    "        \"on\",\n",
    "        \"off\",\n",
    "        \"over\",\n",
    "        \"under\",\n",
    "        \"again\",\n",
    "        \"further\",\n",
    "        \"then\",\n",
    "        \"once\",\n",
    "        \"here\",\n",
    "        \"there\",\n",
    "        \"when\",\n",
    "        \"where\",\n",
    "        \"why\",\n",
    "        \"how\",\n",
    "        \"all\",\n",
    "        \"any\",\n",
    "        \"both\",\n",
    "        \"each\",\n",
    "        \"few\",\n",
    "        \"more\",\n",
    "        \"most\",\n",
    "        \"other\",\n",
    "        \"some\",\n",
    "        \"such\",\n",
    "        \"no\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"only\",\n",
    "        \"own\",\n",
    "        \"same\",\n",
    "        \"so\",\n",
    "        \"than\",\n",
    "        \"too\",\n",
    "        \"very\",\n",
    "        \"s\",\n",
    "        \"t\",\n",
    "        \"can\",\n",
    "        \"will\",\n",
    "        \"just\",\n",
    "        \"don\",\n",
    "        \"don't\",\n",
    "        \"should\",\n",
    "        \"should've\",\n",
    "        \"now\",\n",
    "        \"d\",\n",
    "        \"ll\",\n",
    "        \"m\",\n",
    "        \"o\",\n",
    "        \"re\",\n",
    "        \"ve\",\n",
    "        \"y\",\n",
    "        \"ain\",\n",
    "        \"aren\",\n",
    "        \"aren't\",\n",
    "        \"couldn\",\n",
    "        \"couldn't\",\n",
    "        \"didn\",\n",
    "        \"didn't\",\n",
    "        \"doesn\",\n",
    "        \"doesn't\",\n",
    "        \"hadn\",\n",
    "        \"hadn't\",\n",
    "        \"hasn\",\n",
    "        \"hasn't\",\n",
    "        \"haven\",\n",
    "        \"haven't\",\n",
    "        \"isn\",\n",
    "        \"isn't\",\n",
    "        \"ma\",\n",
    "        \"mightn\",\n",
    "        \"mightn't\",\n",
    "        \"mustn\",\n",
    "        \"mustn't\",\n",
    "        \"needn\",\n",
    "        \"needn't\",\n",
    "        \"shan\",\n",
    "        \"shan't\",\n",
    "        \"shouldn\",\n",
    "        \"shouldn't\",\n",
    "        \"wasn\",\n",
    "        \"wasn't\",\n",
    "        \"weren\",\n",
    "        \"weren't\",\n",
    "        \"won\",\n",
    "        \"won't\",\n",
    "        \"wouldn\",\n",
    "        \"wouldn't\",\n",
    "        \"feeling\",\n",
    "        \"feel\",\n",
    "        \"really\",\n",
    "        \"im\",\n",
    "        \"like\",\n",
    "        \"know\",\n",
    "        \"get\",\n",
    "        \"ive\",\n",
    "        \"im'\",\n",
    "        \"stil\",\n",
    "        \"even\",\n",
    "        \"time\",\n",
    "        \"want\",\n",
    "        \"one\",\n",
    "        \"cant\",\n",
    "        \"think\",\n",
    "        \"go\",\n",
    "        \"much\",\n",
    "        \"never\",\n",
    "        \"day\",\n",
    "        \"back\",\n",
    "        \"see\",\n",
    "        \"still\",\n",
    "        \"make\",\n",
    "        \"thing\",\n",
    "        \"would\",\n",
    "        \"would'\",\n",
    "        \"could'\",\n",
    "        \"little\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_most_common = 350\n",
    "dataset = \"dair_ai_emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(file_path, counter_most_common):\n",
    "\n",
    "    emotion_counters = {emotion: Counter() for emotion in emotions}\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            text, emotion = line.strip().split(\";\")\n",
    "            if emotion in emotions:\n",
    "                words = [\n",
    "                    word\n",
    "                    for word in re.findall(r\"\\w+\", text.lower())\n",
    "                    if word not in stop_words\n",
    "                ]\n",
    "                emotion_counters[emotion].update(words)\n",
    "\n",
    "    emotion_lexicon = {\n",
    "        emotion: [word for word, _ in counter.most_common(counter_most_common)]\n",
    "        for emotion, counter in emotion_counters.items()\n",
    "    }\n",
    "\n",
    "    return emotion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file_path, lexicon):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            text, actual_emotion = line.strip().split(\";\")\n",
    "            words = set(re.findall(r\"\\w+\", text.lower()))\n",
    "\n",
    "            emotion_scores = defaultdict(int)\n",
    "\n",
    "            for emotion, emotion_words in lexicon.items():\n",
    "                common_words = words & emotion_words\n",
    "                emotion_scores[emotion] += len(common_words)\n",
    "\n",
    "            predicted_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "\n",
    "            predictions.append(predicted_emotion)\n",
    "            labels.append(actual_emotion)\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(predictions, labels, num_classes: int):\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average=\"macro\")\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average=\"macro\")\n",
    "    f1_metric = MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    predictions = [emotions.index(prediction) for prediction in predictions]\n",
    "    labels = [emotions.index(label) for label in labels]\n",
    "\n",
    "    predictions = torch.tensor(predictions)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    accuracy = accuracy_metric(predictions, labels)\n",
    "    precision = precision_metric(predictions, labels)\n",
    "    recall = recall_metric(predictions, labels)\n",
    "    f1 = f1_metric(predictions, labels)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = f\"data/{dataset}/train.txt\"\n",
    "test_file_path = f\"data/{dataset}/test.txt\"\n",
    "\n",
    "# Hyperparameters:\n",
    "# increment_step = 5\n",
    "# max_word_count = 2000\n",
    "\n",
    "# for counter_most_common in range(5, max_word_count, increment_step):\n",
    "start = time.time()\n",
    "lexicon = create_lexicon(train_file_path, counter_most_common)\n",
    "train_time = time.time() - start\n",
    "start = time.time()\n",
    "set_lexicon = {\n",
    "    emotion: set(map(str.lower, words)) for emotion, words in lexicon.items()\n",
    "}\n",
    "predictions, labels = predict(test_file_path, set_lexicon)\n",
    "inference_time = time.time() - start\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_results(\n",
    "    predictions, labels, num_classes=len(emotions)\n",
    ")\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [f\"rules_${counter_most_common}most_common_{dataset}\"],\n",
    "        \"dataset\": [dataset],\n",
    "        \"accuracy\": [accuracy.item()],\n",
    "        \"precision\": [precision.item()],\n",
    "        \"recall\": [recall.item()],\n",
    "        \"f1\": [f1.item()],\n",
    "        \"train_time\": [train_time],\n",
    "        \"inference_time\": [inference_time],\n",
    "    }\n",
    ")\n",
    "\n",
    "filename = \"results.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    results.to_csv(filename, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    results.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
